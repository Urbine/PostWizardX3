"""
Bot module (update_mcash_chain) controls the order of execution
of the specialized modules implemented in the ``tasks`` package, which consists of:\n
1. A video dump file capture and web automation scraper\n
2. A photo set source code capture with web automation scraping\n
3. A txt file parser and SQLite database generator.\n
4. An HTML source parser and SQLite database generator.\n

This job control module also includes certain validation steps that make it less
likely to fail in each of the jobs.\n

*** Steps to update the databases from MongerCash ***
(if I were to do it without a centralised job controller)\n
1. Get the dump and source files separately to create the photosets and vid databases.\n
2. Call the parsing modules on the text dump and HTML sources separately.\n
3. Clean the temporary files generated by the entire process.\n
4. Clean outdated files that previous iterations of this process generate.\n

There is a missing step: clean outdated files to avoid file accumulation in the project
directory. There is another workflow in this package that can do it, however, I want to
run the ``clean_outdated_files`` module manually in case I want to check the new and old files.
In my main workflow, I include this step in a bash script that calls ``clean_outdated_files.py``
from this package.

Author: Yoham Gabriel Urbine@GitHub
Email: yohamg@programmer.net

"""

__author__ = "Yoham Gabriel Urbine@GitHub"
__author_email__ = "yohamg@programmer.net"

import argparse
import logging
import os
import sqlite3
import time
import warnings
import tempfile
from tempfile import TemporaryDirectory

# Third-party modules
from rich.console import Console

# Local implementations
from core import (
    is_parent_dir_required,
    get_webdriver,
    get_duration,
    load_from_file,
    clean_filename,
    remove_if_exists,
    update_mcash_conf,
)

from tools.workflows_api import logging_setup, ConsoleStyle
from tasks.mcash_dump_create import get_vid_dump_flow
from tasks.mcash_scrape import get_set_source_flow
from tasks.parse_txt_dump import parse_txt_dump_chain
from tasks.sets_source_parse import db_generate


def cli_arg_updater():
    """
    Process and handle command line arguments for the MongerCash update wizard.
    """

    arg_parser = argparse.ArgumentParser(
        description="mcash local update wizard arguments"
    )

    arg_parser.add_argument(
        "--hint",
        type=str,
        help="This parameter receives the first word of the partner offer for matching",
    )

    arg_parser.add_argument(
        "--parent",
        action="store_true",
        help="Set if you want the resulting files to be located in the parent dir.",
    )

    arg_parser.add_argument(
        "--gecko", action="store_true", help="Use the Gecko webdriver for this process."
    )

    arg_parser.add_argument(
        "--headless", action="store_true", help="Browser headless execution."
    )

    arg_parser.add_argument(
        "--silent", action="store_true", help="Ignore user warnings"
    )

    return arg_parser.parse_args()


if __name__ == "__main__":
    start_time = time.time()
    logging_setup(update_mcash_conf(), __file__)
    logging.info(f"Started Session ID: {os.environ.get('SESSION_ID')}\n")

    console = Console()
    console.print(
        f"Session ID: {os.environ.get('SESSION_ID')}",
        style=ConsoleStyle.TEXT_STYLE_ATTENTION.value,
        justify="left",
    )

    console.print(
        "Welcome to the MongerCash local update wizard\n",
        style=ConsoleStyle.TEXT_STYLE_ACTION.value,
        justify="center",
    )

    args = cli_arg_updater()

    logging.info(f"Passed in {args.__dict__}")

    if args.silent:
        warnings.filterwarnings("ignore")

    temp_dir = tempfile.TemporaryDirectory(dir=".")

    # All webdriver instances have been optimised by the parameter ``no_img``
    # in their ``core.get_webdriver()`` configuration, which means that browser instances
    # won't load images in the web scraping process. In testing, this has shown to
    # generate a performance gain of approximately 50% to 60%.
    # Headless mode tends to be slower and this performance gain does not seem evident,
    # even some seconds slower than NO_IMG_RENDERING = False.
    NO_IMG_RENDERING = True

    webdriver_1 = get_webdriver(
        temp_dir.name,
        headless=args.headless,
        gecko=args.gecko,
        no_imgs=NO_IMG_RENDERING,
    )

    # Fetching
    def fetching_dump_f(wdrv):
        dump_file_name = get_vid_dump_flow(
            wdrv, partner_hint=args.hint, temp_dir_p=temp_dir.name
        )
        return dump_file_name

    with console.status(
        "Getting Dump File from MongerCash. Please wait...", spinner="bouncingBall"
    ):
        dump_f = fetching_dump_f(webdriver_1)

    logging.info(f"Loading dump txt {dump_f} from {temp_dir.name}")

    def load_dump_f(t_dir: TemporaryDirectory, d_file: str):
        return load_from_file(d_file, "txt", dirname=t_dir.name, parent=None)

    load_d_file = load_dump_f(temp_dir, dump_f)
    retry_offset = 3
    retries = 0
    while len(load_d_file) == 0:
        # If the file is empty, it means that something went wrong with the
        # webdriver.
        logging.warning(
            empty_file := "The content of the dump file is empty, retrying..."
        )

        time.sleep(retry_offset)
        logging.info(
            f"Dump fetch - Retry number {retries} | Current offset: {retry_offset}"
        )
        retry_offset += 2
        retries += 1

        webdriver_2 = get_webdriver(
            temp_dir.name,
            headless=args.headless,
            gecko=args.gecko,
            no_imgs=NO_IMG_RENDERING,
        )

        dump_f = fetching_dump_f(webdriver_2)
        load_d_file = load_dump_f(temp_dir, dump_f)

    webdriver_3 = get_webdriver(
        temp_dir.name,
        headless=args.headless,
        gecko=args.gecko,
        no_imgs=NO_IMG_RENDERING,
    )

    with console.status(
        "Getting all available Photo Sets from MongerCash. Please wait...",
        spinner="bouncingBall",
    ):
        photoset_source = get_set_source_flow(webdriver_3, partner_hint=args.hint)

    while len(photoset_source[0]) == 0:
        logging.info(photo_fail := "The source file is empty, retrying...")
        warnings.warn(photo_fail, UserWarning)

        time.sleep(retry_offset)
        retry_offset += 2
        logging.info(
            f"Photo set fetch - Retry number {retries} | Current offset: {retry_offset}"
        )
        retries += 1

        webdriver_4 = get_webdriver(
            temp_dir.name,
            headless=args.headless,
            gecko=args.gecko,
            no_imgs=NO_IMG_RENDERING,
        )

        dump_f = fetching_dump_f(webdriver_4)
        load_d_file = load_dump_f(temp_dir, dump_f)

    db_name = clean_filename(dump_f, "db")
    db_path = os.path.join(is_parent_dir_required(parent=args.parent), db_name)
    remove_if_exists(db_path)
    db_conn = sqlite3.connect(db_path)
    cursor = db_conn.cursor()
    logging.info(f"Created database {db_name} at {db_path}")
    cursor.execute("BEGIN TRANSACTION")
    cursor.execute(
        """
    CREATE TABLE
        videos(
        title,
        description,
        model,
        tags,
        date,
        duration,
        source_url,
        thumbnail_url,
        tracking_url,
        wp_slug
        )
    """
    )

    parsing = parse_txt_dump_chain(
        dump_f,
        db_name,
        db_conn,
        cursor,
        dirname=temp_dir.name,
        parent=args.parent,
    )

    logging.info(
        vid_result
        := f"{parsing[1]} video entries have been processed from {dump_f} and inserted into\n{parsing[0]}\n"
    )
    console.print(
        vid_result, style=ConsoleStyle.TEXT_STYLE_ATTENTION.value, justify="left"
    )

    parsing_photos = db_generate(
        photoset_source[0], photoset_source[1], parent=args.parent
    )

    logging.info(
        photo_result
        := f"{parsing_photos[1]} photo set entries have been processed and inserted into\n{parsing_photos[0]}\n"
    )
    console.print(
        photo_result, style=ConsoleStyle.TEXT_STYLE_ATTENTION.value, justify="left"
    )

    logging.info(tidy_up := f"Cleaning temporary directory {temp_dir.name}")
    console.print(
        tidy_up, style=ConsoleStyle.TEXT_STYLE_ATTENTION.value, justify="left"
    )
    temp_dir.cleanup()

    end_time = time.time()
    hours, mins, secs = get_duration(end_time - start_time)
    logging.info(f"Process took: Hours: {hours} Mins: {mins} Secs: {secs}")
    logging.shutdown()
