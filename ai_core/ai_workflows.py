"""
AI Workflows Module

This module provides AI-driven workflows for content processing, combining computer vision and
large language models to automate content creation and optimization tasks. It coordinates
multiple AI systems to transform visual media into publication-ready content packages.

Key features:
- Orchestrates end-to-end AI content processing workflows
- Extracts and captions video frames using BLIP image-captioning model
- Generates SEO-optimized metadata through LLM integration
- Creates comprehensive content attributes for publishing platforms
- Enhances SEO performance by incorporating search trend data
- Prepares structured outputs for WordPress and other CMS integration
- Parallelizes processing using asyncio and threading
- Leverages GPU acceleration when available

Dependencies:
- OpenCV (cv2) for video processing
- PyTorch and Transformers for computer vision models
- LMStudio client for large language model integration
- PIL for image processing
- Google Search and WordPress API integrations
- Threading and asyncio for concurrent workflow execution

Author: Yoham Gabriel GitHub@Urbine
Email: yohamg@programmer.net
"""

__author__ = "Yoham Gabriel Urbine@GitHub"
__author_email__ = "yohamg@programmer.net"

import asyncio
import threading

from collections import deque
from pathlib import Path
from typing import Optional

# Third-party libraries
import cv2
import lmstudio as lms
import torch
from PIL import Image
from transformers import AutoProcessor, BlipForConditionalGeneration

from ai_core.ai_client_mgr import load_llm_model

# Local implementations
from .config.ai_config import get_inference_params
from integrations import google_search
from ml_engine import classify_title, classify_description, classify_tags


DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")


processor = AutoProcessor.from_pretrained(
    "Salesforce/blip-image-captioning-base", use_fast=True
)
model = BlipForConditionalGeneration.from_pretrained(
    "Salesforce/blip-image-captioning-base"
)
model.to(DEVICE)


caption_lock = threading.Lock()
semaphore = asyncio.Semaphore(10)
captions = deque()

video_path = ""


def video_frame_captions(video_path):
    """
    Captures all frames from a given video file.

    This function takes a path to a video file as input and returns a list of frames captured from it.
    Each frame is returned as a NumPy array of shape (height, width, channels), where height, width are the dimensions
    of the image and channels represent the number of color channels (e.g., 3 for RGB).

    :param video_path: str - The path to the video file.
    :return: list - A list of NumPy arrays representing the frames of the video.
    """
    frame_list = []
    cap = cv2.VideoCapture(video_path)
    frame_num = 0
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        frame_list.append(frame)
        frame_num += 1
    cap.release()
    return frame_list


def get_captions_list(img):
    """
    get_captions_list is a function that takes an image as input,
    processes it using a pre-trained model and its associated processor,
    and generates caption text for the image.
    The generated captions are returned in a list.

    :param img: PIL.Image - The image to be processed. It should be a PIL Image object or any other compatible format.
    :return: list - A list of strings, where each string is a caption generated by the model for the given image.
    """
    inputs = processor(img, return_tensors="pt").to(DEVICE)
    outputs = model.generate(**inputs)
    captions = processor.decode(outputs[0], skip_special_tokens=True)
    return captions


def video_caption(frame, nparray_img=False):
    if nparray_img:
        image = Image.fromarray(frame)
    else:
        image = Image.open(frame)

    async def generate_captions():
        async with semaphore:
            caption = get_captions_list(image)
            if caption not in captions:
                try:
                    if caption_lock.acquire(timeout=10):
                        captions.append(caption)
                finally:
                    caption_lock.release()

    async def main():
        cap_tasks = [generate_captions() for _ in video_frame_captions(video_path)]
        await asyncio.gather(*cap_tasks)

        asyncio.run(main())


def ai_video_attrs(
    img: Path,
    title: str,
    description: Optional[str],
    tags: Optional[str],
    ai_model: lms.LLM,
    override_prompt: str = None,
):
    class AttrsModel(lms.BaseModel):
        alt_text: str
        caption: str
        description: str
        slug: str
        tags: list[str]
        category: str

    image_file = Image.open(img)
    caption = get_captions_list(image_file)
    google = google_search.GoogleImage()

    classifiers = [
        classify_title(title),
        classify_description(description) if description else "",
        classify_tags(tags) if tags else "",
    ]

    categories = {categ for categs in classifiers for categ in categs if categ}
    google_results = google.get_serp_items(title)

    if not override_prompt:
        prompt = f""" 
        You are a specialised and professional SEO expert in the adult industry and you are processing
        outputs for SERP optimisation and educational purposes. 
        Your task is to generate a set of attributes for a video thumbnail:
        1. ALT Text
        2. Caption
        3. Description
        4. SEO Friendly and Optimised Slug for both the image file and post
        5. Optimized tags for the post (based on what you see in the search results) without separators, example: "some tag"
        6. Pick from the list provided the most suitable category for the post.
        You will be given: 
        -> Title: {title} 
        -> Description: {description} 
        -> Image caption: {caption} 
        -> List of categories to choose from: {categories}
        -> Google results: {google_results} 
        Use the Google search results to help you generate the details and use them to improve the description.
        Differentiate this posts from others you see in the results from Google Search, so that this post can rank.
        In case the description is `None`, you should generate a description based on the image caption and results in a creative tone.
        Give me the results in JSON format with the following keys:
        alt_text, caption, description, category, slug, tags
        """
    else:
        prompt = override_prompt

    process = ai_model.respond(
        prompt=prompt,
        response_format=AttrsModel,
        config=get_inference_params(),
    )

    structured_response = process.parsed
    alt_text = structured_response["alt_text"]
    description = structured_response["description"]
    slug = structured_response["slug"].strip("/")
    tags = structured_response["tags"]
    category = structured_response["category"]

    return alt_text, caption, description, category, slug, tags


if __name__ == "__main__":
    # Testing code
    from core import get_duration
    import time

    llm, _ = load_llm_model()
    print("LLM loaded")
    time_start = time.time()
    ai_act = ai_video_attrs(
        Path(""),
        "",
        "",
        None,
        llm,
    )
    print(ai_act)
    time_end = time.time()
    h, mins, secs = get_duration(time_end - time_start)
    print("This process took: ", "hours:", h, "mins:", mins, "secs:", secs)
    llm.unload()
